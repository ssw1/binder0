{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Policy Example\n",
    "\n",
    "## Overview\n",
    "\n",
    "The environment is 6x6 grid with no obstacles.\n",
    "One agent tries to reach its target starting from the (0, 0) cell.\n",
    "\n",
    "## Algorithm Outline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from visual import Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "The Q-table is updated and optimized step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the example\n",
    "Build the environment, and run the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grid.Grid object at 0x0000021AE6EE6AC0>\n",
      "Policy A\n",
      "D    U    R    L    D    R    \n",
      "D    D    D    L    U    L    \n",
      "L    U    D    R    L    U    \n",
      "U    U    D    R    U    L    \n",
      "D    L    R    R    D    L    \n",
      "R    L    R    U              \n",
      "\n",
      "Iteration: 1  Biggest change: 1.000\n",
      "Iteration: 2  Biggest change: 0.900\n",
      "Iteration: 3  Biggest change: 0.810\n",
      "Iteration: 4  Biggest change: 0.729\n",
      "Iteration: 5  Biggest change: 0.656\n",
      "Iteration: 6  Biggest change: 0.590\n",
      "Iteration: 7  Biggest change: 0.000\n",
      "\n",
      "Value\n",
      "   S   0.00  0.00  0.00  0.00  0.00 \n",
      " 0.00  0.00 -0.59 -0.53  0.00  0.00 \n",
      " 0.00  0.00 -0.66  0.00  0.00  0.00 \n",
      " 0.00  0.00 -0.73  0.00  0.00  0.00 \n",
      " 0.00  0.00 -0.81 -0.90 -1.00 -0.90 \n",
      " 0.00  0.00 -0.73 -0.81    L     W  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from grid import Grid\n",
    "from agent import DPAgentGrid\n",
    "\n",
    "# Define a 6x6 grid\n",
    "env = Grid(6)\n",
    "print(env)\n",
    "# Instantiate two agents with their starting points, destinations, and 'traps'.\n",
    "agent1 = DPAgentGrid('A', env, (0, 0), (5, 5), (5, 4))\n",
    "agent2 = DPAgentGrid('B', env, (5, 0), (0, 5), (1, 5))\n",
    "\n",
    "# Initialize policy. Each state will have a value.\n",
    "agent1.init_policy()\n",
    "agent2.init_policy()\n",
    "\n",
    "# Improve policy for agent 1.\n",
    "agent1.improve_policy(True)\n",
    "agent1.show_policy()\n",
    "agent1.show_V()\n",
    "\n",
    "# Improve policy for agent 2.\n",
    "agent2.improve_policy(False)\n",
    "agent2.show_policy()\n",
    "agent2.show_V()\n",
    "\n",
    "\n",
    "# Show results.\n",
    "agent1.show_policy()\n",
    "agent2.show_policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "converged = env.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
